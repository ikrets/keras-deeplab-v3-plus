{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "if 'keras-deeplab-v3-plus' not in sys.path:\n",
    "    sys.path.append('keras-deeplab-v3-plus')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import relu6, BilinearUpsampling, Deeplabv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = Deeplabv3(weights=None, \n",
    "                      OS=8, \n",
    "                      input_shape=(256, 640, 3), \n",
    "                      backbone='mobilenetv2', \n",
    "                      alpha=1.0, \n",
    "                      classes=2,\n",
    "                      weight_decay=0.0001,\n",
    "                      dropout=0.1)\n",
    "\n",
    "model_new.load_weights('/home/ubuntu/hopefully_right_starting_point.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dataset import MyDataset\n",
    "\n",
    "train_dataset = MyDataset(filename='/home/ubuntu/train1.tfrecord',\n",
    "                              batch_size=4,\n",
    "                              shape=(256, 640),\n",
    "                              num_readers=4,\n",
    "                              num_classes=2,\n",
    "                              is_training=True,\n",
    "                              should_shuffle=True,\n",
    "                              should_repeat=True,\n",
    "                              should_augment=True).get()\n",
    "\n",
    "val_dataset = MyDataset(filename='/home/ubuntu/val1.tfrecord',\n",
    "                       batch_size=4,\n",
    "                       shape=(256, 640),\n",
    "                       num_readers=4,\n",
    "                       num_classes=2,\n",
    "                       is_training=True,\n",
    "                       should_shuffle=True,\n",
    "                       should_repeat=True,\n",
    "                       should_augment=False).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((?, ?, ?, 3), (?, 256, 640, 2)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss(class_weights):\n",
    "    num_classes = len(class_weights)\n",
    "    def loss(labels, pred):\n",
    "        weight_masks = []\n",
    "        for i in range(num_classes):\n",
    "            weight_masks.append(tf.to_float(tf.equal(tf.argmax(labels, axis=-1), i)) * class_weights[i])\n",
    "\n",
    "        return tf.losses.softmax_cross_entropy(\n",
    "            labels,\n",
    "            pred,\n",
    "            weights=tf.add_n(weight_masks))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(lr=0.00001)\n",
    "model_new.compile(optimizer=adam, loss=make_loss([1., 50.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 13s 724ms/step - loss: 0.4781 - val_loss: 0.5465\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 0.4770 - val_loss: 0.6073\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 0.4761 - val_loss: 0.5876\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 12s 670ms/step - loss: 0.4754 - val_loss: 0.5485\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 12s 668ms/step - loss: 0.4764 - val_loss: 0.6142\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 12s 672ms/step - loss: 0.4744 - val_loss: 0.5459\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 0.4762 - val_loss: 0.5625\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 0.4749 - val_loss: 0.5878\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 0.4733 - val_loss: 0.5914\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4755 - val_loss: 0.5614\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 12s 674ms/step - loss: 0.4749 - val_loss: 0.5945\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4732 - val_loss: 0.5415\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.4719 - val_loss: 0.6023\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4720 - val_loss: 0.5726\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 0.4722 - val_loss: 0.5713\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.4728 - val_loss: 0.5712\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 0.4715 - val_loss: 0.5682\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4713 - val_loss: 0.5435\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 0.4706 - val_loss: 0.5470\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 0.4709 - val_loss: 0.5833\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 12s 683ms/step - loss: 0.4706 - val_loss: 0.5720\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 0.4709 - val_loss: 0.5758\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.4697 - val_loss: 0.5599\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 0.4692 - val_loss: 0.5739\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4695 - val_loss: 0.5711\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4695 - val_loss: 0.5711\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 12s 684ms/step - loss: 0.4687 - val_loss: 0.5838\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4691 - val_loss: 0.5561\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 12s 682ms/step - loss: 0.4672 - val_loss: 0.5848\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 12s 683ms/step - loss: 0.4679 - val_loss: 0.5604\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 0.4682 - val_loss: 0.5366\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4667 - val_loss: 0.5614\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4669 - val_loss: 0.5881\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4665 - val_loss: 0.5597\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 0.4667 - val_loss: 0.5532\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 0.4651 - val_loss: 0.5591\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4654 - val_loss: 0.5455\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4658 - val_loss: 0.5899\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 12s 683ms/step - loss: 0.4666 - val_loss: 0.5661\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 0.4654 - val_loss: 0.5675\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.4645 - val_loss: 0.5517\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 0.4643 - val_loss: 0.5856\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4635 - val_loss: 0.5406\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4646 - val_loss: 0.5696\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.4634 - val_loss: 0.5948\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4637 - val_loss: 0.5465\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4641 - val_loss: 0.5651\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4613 - val_loss: 0.5858\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4621 - val_loss: 0.5379\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 12s 682ms/step - loss: 0.4625 - val_loss: 0.5600\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4607 - val_loss: 0.5622\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4626 - val_loss: 0.5636\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 0.4612 - val_loss: 0.5507\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 0.4616 - val_loss: 0.5630\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 0.4598 - val_loss: 0.5894\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4613 - val_loss: 0.5364\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4596 - val_loss: 0.6191\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4595 - val_loss: 0.5516\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4599 - val_loss: 0.5464\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.4592 - val_loss: 0.5705\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4587 - val_loss: 0.5932\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.4598 - val_loss: 0.5781\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4598 - val_loss: 0.5768\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4571 - val_loss: 0.5422\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4578 - val_loss: 0.5664\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4584 - val_loss: 0.5840\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4562 - val_loss: 0.5311\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 0.4565 - val_loss: 0.5802\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4558 - val_loss: 0.5624\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4575 - val_loss: 0.5416\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4561 - val_loss: 0.5553\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 12s 673ms/step - loss: 0.4552 - val_loss: 0.5931\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.4550 - val_loss: 0.5676\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 0.4548 - val_loss: 0.5677\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4539 - val_loss: 0.5676\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.4543 - val_loss: 0.5401\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4538 - val_loss: 0.5668\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4536 - val_loss: 0.5971\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4534 - val_loss: 0.5736\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 0.4542 - val_loss: 0.5476\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4529 - val_loss: 0.5593\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4528 - val_loss: 0.5645\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4527 - val_loss: 0.5390\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 0.4530 - val_loss: 0.5601\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4528 - val_loss: 0.5468\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4529 - val_loss: 0.5706\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 12s 674ms/step - loss: 0.4509 - val_loss: 0.5760\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4515 - val_loss: 0.6006\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 12s 678ms/step - loss: 0.4501 - val_loss: 0.5520\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4494 - val_loss: 0.5341\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 12s 681ms/step - loss: 0.4507 - val_loss: 0.5709\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 12s 680ms/step - loss: 0.4502 - val_loss: 0.5597\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4496 - val_loss: 0.5696\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4495 - val_loss: 0.5690\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.4488 - val_loss: 0.5775\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 12s 675ms/step - loss: 0.4498 - val_loss: 0.5554\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 12s 674ms/step - loss: 0.4492 - val_loss: 0.5640\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4492 - val_loss: 0.5440\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 12s 677ms/step - loss: 0.4481 - val_loss: 0.5618\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 12s 679ms/step - loss: 0.4476 - val_loss: 0.5694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f79469d5160>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.fit(train_dataset, epochs=100, steps_per_epoch=round(72/4),\n",
    "             validation_data=val_dataset,\n",
    "             validation_steps=round(25/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.save('depth1_color_aug_val054-059.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array(Image.open('/home/ubuntu/dataset/images/home23_02_2019-02-23-11-55-12_0_frame0671.jpg'))\n",
    "sample = (sample * 2.0 / 255.0) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = '/home/ubuntu/2019-02-23-11-55-12_0/'\n",
    "evals = [os.path.join(eval_dir, f) for f in os.listdir(eval_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.4\n",
    "eval_out_dir = '/home/ubuntu/eval_2019-02-23-11-55-12_0/'\n",
    "if not os.path.exists(eval_out_dir):\n",
    "    os.makedirs(eval_out_dir)\n",
    "\n",
    "for e in list(os.listdir(eval_dir)):\n",
    "    i = np.array(Image.open(os.path.join(eval_dir, e)))\n",
    "    i = i[-256:, :, :]\n",
    "    pred = model_new.predict(np.expand_dims(i * 2.0 / 255.0 - 1.0, axis=0))\n",
    "    \n",
    "    pred = np.argmax(np.squeeze(pred), axis=2)\n",
    "    i[pred == 1] = alpha * np.array([0, 255, 0]) + (1 - alpha) * i[pred == 1] \n",
    "    \n",
    "    i = Image.fromarray(i)\n",
    "    i.save(os.path.join(eval_out_dir, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(pred):\n",
    "    pred = np.squeeze(pred)\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    \n",
    "\n",
    "    plt.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd8bbc6dba8>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true = np.array(Image.open('/home/ubuntu/dataset/masks/home23_02_2019-02-23-11-55-12_0_frame0671.png'))\n",
    "plt.imshow(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
